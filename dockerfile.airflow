#Apache Airflow base image
FROM apache/airflow:2.7.1-python3.11

USER root

#system dependencies
RUN apt-get update && \
    apt-get install -y procps && \
    apt-get install -y gcc python3-dev openjdk-11-jdk && \
    apt-get clean
    
#Download&installation of Spark
ENV SPARK_VERSION=3.2.1
ENV HADOOP_VERSION=3.2

RUN curl -O https://archive.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz && \
    tar xf spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz -C /usr/local && \
    rm spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz && \
    ln -s /usr/local/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION /usr/local/spark


#environment variables for Spark
ENV SPARK_HOME=/usr/local/spark
ENV PATH=$PATH:$SPARK_HOME/bin

USER airflow

#Install Apache Airflow and other Python dependencies
COPY requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt

#necessary Airflow ports
EXPOSE 8080 5555 8793

WORKDIR /opt/airflow

CMD ["bash"]

